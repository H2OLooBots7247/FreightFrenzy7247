Index: TeamCode/src/main/java/org/firstinspires/ftc/teamcode/CameraStreamTest.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>package org.firstinspires.ftc.teamcode;\r\n\r\nimport com.acmerobotics.dashboard.FtcDashboard;\r\nimport com.qualcomm.robotcore.eventloop.opmode.LinearOpMode;\r\nimport com.qualcomm.robotcore.eventloop.opmode.TeleOp;\r\n\r\nimport org.firstinspires.ftc.robotcore.external.ClassFactory;\r\nimport org.firstinspires.ftc.robotcore.external.navigation.VuforiaLocalizer;\r\n\r\n@TeleOp\r\npublic class CameraStreamTest extends LinearOpMode {\r\n\r\n    public static final String VUFORIA_LICENSE_KEY = \"AUhZBUP/////AAABmYEGpdLRPksVnc0ztTr0AVMWkvz/IqsD3cuBMKME0ZRQfnHZVGjZvnw138iHecuD+jNRvjNyidYb2ZgXwzaSru+n6xtkfyQvN7GU2s/kXkxMtJm5EGwMUkDqULQCEnqtm68Cc0FfKCV+aygL1qRRMHwfttGd82y5GqqnaEejg9Ummb/e7tGIaHsSlQJ9Met3Wwo9CzXCMZUa+SOq2orh0b2dv0Gj0xi4vzjBKdllxE6aXRYgXfq2h7Nxnx3MrdgnyUTn5FEJicPbXU4knlZEXE2+qSSmMeCaXw4KzSF/e5nDilQYgTYxRqE06Qzu1t0xqZQsIHnAdkFjmEdLpFwePjthqUUl2mRr7jGCNqZgmH1u\";\r\n\r\n    @Override\r\n    public void runOpMode() throws InterruptedException {\r\n        // gives Vuforia more time to exit before the watchdog notices\r\n        msStuckDetectStop = 2500;\r\n\r\n        VuforiaLocalizer.Parameters vuforiaParams = new VuforiaLocalizer.Parameters(R.id.cameraMonitorViewId);\r\n        vuforiaParams.vuforiaLicenseKey = VUFORIA_LICENSE_KEY;\r\n        vuforiaParams.cameraDirection = VuforiaLocalizer.CameraDirection.BACK;\r\n        VuforiaLocalizer vuforia = ClassFactory.getInstance().createVuforia(vuforiaParams);\r\n\r\n        FtcDashboard.getInstance().startCameraStream(vuforia, 0);\r\n\r\n        waitForStart();\r\n\r\n        while (opModeIsActive());\r\n    }\r\n\r\n}\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/TeamCode/src/main/java/org/firstinspires/ftc/teamcode/CameraStreamTest.java b/TeamCode/src/main/java/org/firstinspires/ftc/teamcode/CameraStreamTest.java
--- a/TeamCode/src/main/java/org/firstinspires/ftc/teamcode/CameraStreamTest.java	(revision e88ab475f7abf8372d216d278ad86bc2d6146a80)
+++ b/TeamCode/src/main/java/org/firstinspires/ftc/teamcode/CameraStreamTest.java	(date 1635107603836)
@@ -22,11 +22,15 @@
         vuforiaParams.cameraDirection = VuforiaLocalizer.CameraDirection.BACK;
         VuforiaLocalizer vuforia = ClassFactory.getInstance().createVuforia(vuforiaParams);
 
-        FtcDashboard.getInstance().startCameraStream(vuforia, 0);
+        FtcDashboard.getInstance().startCameraStream(vuforia, 24);
 
         waitForStart();
 
-        while (opModeIsActive());
+        while (opModeIsActive()) {
+
+            FtcDashboard.getInstance().startCameraStream(vuforia, 24);
+
+        }
     }
 
 }
Index: TeamCode/src/main/java/org/firstinspires/ftc/teamcode/Auto1.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>package org.firstinspires.ftc.teamcode;\r\n\r\nimport com.qualcomm.robotcore.eventloop.opmode.Autonomous;\r\nimport com.qualcomm.robotcore.eventloop.opmode.LinearOpMode;\r\n\r\nimport org.ftc.waterloo.h2oloobots.AttachmentControl;\r\nimport org.ftc.waterloo.h2oloobots.DriveTrain;\r\n\r\n@Autonomous(name = \"Auto1\", group = \"Group\")\r\npublic class Auto1 extends LinearOpMode {\r\n\r\n    DriveTrain driveTrain = new DriveTrain();\r\n    AttachmentControl attachmentControl = new AttachmentControl();\r\n\r\n    double SPEED = 0.5;\r\n\r\n    public void runOpMode() {\r\n\r\n        driveTrain.FourMotorInit(true, hardwareMap);\r\n        attachmentControl.attachmentInit(hardwareMap, telemetry, 1);\r\n\r\n        driveTrain.EncoderAutoInit(100, 26.9, 28);\r\n\r\n        waitForStart();\r\n\r\n\r\n        driveTrain.EncoderAutoMecanumDrive(12, 0, 0, 0.5, 5);\r\n\r\n        telemetry.addData(\"Front Right Target Position\", driveTrain.fr.getTargetPosition());\r\n        telemetry.addData(\"Front Left Target Position\", driveTrain.fl.getTargetPosition());\r\n        telemetry.addData(\"Back Right Target Position\", driveTrain.br.getTargetPosition());\r\n        telemetry.addData(\"Back Left Target Position\", driveTrain.bl.getTargetPosition());\r\n        telemetry.update();\r\n\r\n        driveTrain.EncoderAutoMecanumDrive(0, 12, 0, 0.5, 5);\r\n\r\n        telemetry.addData(\"Front Right Target Position\", driveTrain.fr.getTargetPosition());\r\n        telemetry.addData(\"Front Left Target Position\", driveTrain.fl.getTargetPosition());\r\n        telemetry.addData(\"Back Right Target Position\", driveTrain.br.getTargetPosition());\r\n        telemetry.addData(\"Back Left Target Position\", driveTrain.bl.getTargetPosition());\r\n        telemetry.update();\r\n\r\n        driveTrain.EncoderAutoMecanumDrive(0, 0, 12, 0.5, 5);\r\n\r\n        telemetry.addData(\"Front Right Target Position\", driveTrain.fr.getTargetPosition());\r\n        telemetry.addData(\"Front Left Target Position\", driveTrain.fl.getTargetPosition());\r\n        telemetry.addData(\"Back Right Target Position\", driveTrain.br.getTargetPosition());\r\n        telemetry.addData(\"Back Left Target Position\", driveTrain.bl.getTargetPosition());\r\n        telemetry.update();\r\n\r\n        driveTrain.EncoderAutoMecanumDrive(0, 0, -12, 0.5, 5);\r\n\r\n        telemetry.addData(\"Front Right Target Position\", driveTrain.fr.getTargetPosition());\r\n        telemetry.addData(\"Front Left Target Position\", driveTrain.fl.getTargetPosition());\r\n        telemetry.addData(\"Back Right Target Position\", driveTrain.br.getTargetPosition());\r\n        telemetry.addData(\"Back Left Target Position\", driveTrain.bl.getTargetPosition());\r\n        telemetry.update();\r\n\r\n        driveTrain.EncoderAutoMecanumDrive(0, -12, 0, 0.5, 5);\r\n\r\n        telemetry.addData(\"Front Right Target Position\", driveTrain.fr.getTargetPosition());\r\n        telemetry.addData(\"Front Left Target Position\", driveTrain.fl.getTargetPosition());\r\n        telemetry.addData(\"Back Right Target Position\", driveTrain.br.getTargetPosition());\r\n        telemetry.addData(\"Back Left Target Position\", driveTrain.bl.getTargetPosition());\r\n        telemetry.update();\r\n\r\n        driveTrain.EncoderAutoMecanumDrive(-12, 0, 0, 0.5, 5);\r\n\r\n        telemetry.addData(\"Front Right Target Position\", driveTrain.fr.getTargetPosition());\r\n        telemetry.addData(\"Front Left Target Position\", driveTrain.fl.getTargetPosition());\r\n        telemetry.addData(\"Back Right Target Position\", driveTrain.br.getTargetPosition());\r\n        telemetry.addData(\"Back Left Target Position\", driveTrain.bl.getTargetPosition());\r\n        telemetry.update();\r\n\r\n        sleep(10000);\r\n\r\n    }\r\n\r\n}\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/TeamCode/src/main/java/org/firstinspires/ftc/teamcode/Auto1.java b/TeamCode/src/main/java/org/firstinspires/ftc/teamcode/Auto1.java
--- a/TeamCode/src/main/java/org/firstinspires/ftc/teamcode/Auto1.java	(revision e88ab475f7abf8372d216d278ad86bc2d6146a80)
+++ b/TeamCode/src/main/java/org/firstinspires/ftc/teamcode/Auto1.java	(date 1635107914694)
@@ -1,11 +1,13 @@
 package org.firstinspires.ftc.teamcode;
 
+import com.acmerobotics.dashboard.config.Config;
 import com.qualcomm.robotcore.eventloop.opmode.Autonomous;
 import com.qualcomm.robotcore.eventloop.opmode.LinearOpMode;
 
 import org.ftc.waterloo.h2oloobots.AttachmentControl;
 import org.ftc.waterloo.h2oloobots.DriveTrain;
 
+@Config
 @Autonomous(name = "Auto1", group = "Group")
 public class Auto1 extends LinearOpMode {
 
Index: TeamCode/src/main/java/org/firstinspires/ftc/teamcode/Tele1.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>package org.firstinspires.ftc.teamcode;\r\n\r\nimport com.qualcomm.robotcore.eventloop.opmode.LinearOpMode;\r\nimport com.qualcomm.robotcore.eventloop.opmode.TeleOp;\r\n\r\nimport org.firstinspires.ftc.robotcore.external.Telemetry;\r\nimport org.ftc.waterloo.h2oloobots.AttachmentControl;\r\nimport org.ftc.waterloo.h2oloobots.DriveTrain;\r\nimport org.ftc.waterloo.h2oloobots.TelemetryControl;\r\n\r\n@TeleOp\r\npublic class Tele1 extends LinearOpMode {\r\n\r\n    public DriveTrain driveTrain = new DriveTrain();\r\n    public AttachmentControl attachmentControl = new AttachmentControl();\r\n    public TelemetryControl telemetryControl = new TelemetryControl();\r\n\r\n    int waitForTelemetry = 0;\r\n\r\n    public void runOpMode() {\r\n\r\n        driveTrain.FourMotorInit(false, hardwareMap);\r\n        attachmentControl.attachmentInit(hardwareMap, telemetry, 1);\r\n\r\n        waitForStart();\r\n\r\n        while (opModeIsActive()) {\r\n\r\n            driveTrain.MecanumTeleOp(gamepad1.left_stick_y, gamepad1.left_stick_x, gamepad1.right_stick_x, false, telemetry);\r\n            attachmentControl.duckMotorTeleop(gamepad1.x);\r\n            attachmentControl.liftMotorMove(gamepad1.y, gamepad1.a);\r\n\r\n            telemetry.addData(\"Front Left Motor Power\", driveTrain.fl.getPower());\r\n            telemetry.addData(\"Front Right Motor Power\", driveTrain.fr.getPower());\r\n            telemetry.addData(\"Back Left Motor Power\", driveTrain.bl.getPower());\r\n            telemetry.addData(\"Back Right Motor Power\", driveTrain.br.getPower());\r\n            telemetry.addData(\"Lift Motor Position\", attachmentControl.LiftMotor.getCurrentPosition());\r\n            telemetry.addData(\"Duck Motor Power\", attachmentControl.DuckMotor.getPower());\r\n            telemetry.update();\r\n\r\n        }\r\n\r\n    }\r\n\r\n}\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/TeamCode/src/main/java/org/firstinspires/ftc/teamcode/Tele1.java b/TeamCode/src/main/java/org/firstinspires/ftc/teamcode/Tele1.java
--- a/TeamCode/src/main/java/org/firstinspires/ftc/teamcode/Tele1.java	(revision e88ab475f7abf8372d216d278ad86bc2d6146a80)
+++ b/TeamCode/src/main/java/org/firstinspires/ftc/teamcode/Tele1.java	(date 1635107914684)
@@ -1,5 +1,6 @@
 package org.firstinspires.ftc.teamcode;
 
+import com.acmerobotics.dashboard.config.Config;
 import com.qualcomm.robotcore.eventloop.opmode.LinearOpMode;
 import com.qualcomm.robotcore.eventloop.opmode.TeleOp;
 
@@ -9,6 +10,7 @@
 import org.ftc.waterloo.h2oloobots.TelemetryControl;
 
 @TeleOp
+@Config
 public class Tele1 extends LinearOpMode {
 
     public DriveTrain driveTrain = new DriveTrain();
Index: TeamCode/src/main/java/org/camera/openftc/easyopencv/FreightDeterminationExample.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>/*\r\n * Copyright (c) 2020 OpenFTC Team\r\n *\r\n * Permission is hereby granted, free of charge, to any person obtaining a copy\r\n * of this software and associated documentation files (the \"Software\"), to deal\r\n * in the Software without restriction, including without limitation the rights\r\n * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\r\n * copies of the Software, and to permit persons to whom the Software is\r\n * furnished to do so, subject to the following conditions:\r\n *\r\n * The above copyright notice and this permission notice shall be included in all\r\n * copies or substantial portions of the Software.\r\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\r\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\r\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\r\n * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\r\n * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\r\n * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\r\n * SOFTWARE.\r\n */\r\n\r\npackage org.camera.openftc.easyopencv;\r\n\r\nimport com.qualcomm.robotcore.eventloop.opmode.Disabled;\r\nimport com.qualcomm.robotcore.eventloop.opmode.LinearOpMode;\r\nimport com.qualcomm.robotcore.eventloop.opmode.TeleOp;\r\n\r\nimport org.firstinspires.ftc.robotcore.external.hardware.camera.WebcamName;\r\nimport org.opencv.core.Core;\r\nimport org.opencv.core.Mat;\r\nimport org.opencv.core.Point;\r\nimport org.opencv.core.Rect;\r\nimport org.opencv.core.Scalar;\r\nimport org.opencv.imgproc.Imgproc;\r\nimport org.openftc.easyopencv.OpenCvCamera;\r\nimport org.openftc.easyopencv.OpenCvCameraFactory;\r\nimport org.openftc.easyopencv.OpenCvCameraRotation;\r\nimport org.openftc.easyopencv.OpenCvPipeline;\r\nimport org.openftc.easyopencv.OpenCvWebcam;\r\n\r\n/*\r\n * This sample demonstrates a basic (but battle-tested and essentially\r\n * 100% accurate) method of detecting the Freight when lined up with\r\n * the sample regions over the first 3 stones.\r\n */\r\n@TeleOp\r\n@Disabled\r\npublic class FreightDeterminationExample extends LinearOpMode\r\n{\r\n    OpenCvWebcam webcam;\r\n    FreightDeterminationPipeline pipeline;\r\n\r\n    @Override\r\n    public void runOpMode()\r\n    {\r\n        /**\r\n         * NOTE: Many comments have been omitted from this sample for the\r\n         * sake of conciseness. If you're just starting out with EasyOpenCv,\r\n         * you should take a look at {@link InternalCamera1Example} or its\r\n         * webcam counterpart, {@link WebcamExample} first.\r\n         */\r\n\r\n        int cameraMonitorViewId = hardwareMap.appContext.getResources().getIdentifier(\"cameraMonitorViewId\", \"id\", hardwareMap.appContext.getPackageName());\r\n        webcam = OpenCvCameraFactory.getInstance().createWebcam(hardwareMap.get(WebcamName.class, \"Webcam 1\"), cameraMonitorViewId);\r\n        pipeline = new FreightDeterminationPipeline();\r\n        webcam.setPipeline(pipeline);\r\n\r\n        // We set the viewport policy to optimized view so the preview doesn't appear 90 deg\r\n        // out when the RC activity is in portrait. We do our actual image processing assuming\r\n        // landscape orientation, though.\r\n\r\n        webcam.setMillisecondsPermissionTimeout(2500);\r\n        webcam.openCameraDeviceAsync(new OpenCvCamera.AsyncCameraOpenListener()\r\n        {\r\n            @Override\r\n            public void onOpened()\r\n            {\r\n                webcam.startStreaming(320,240, OpenCvCameraRotation.UPRIGHT);\r\n            }\r\n\r\n            @Override\r\n            public void onError(int errorCode)\r\n            {\r\n                /*\r\n                 * This will be called if the camera could not be opened\r\n                 */\r\n            }\r\n        });\r\n\r\n        waitForStart();\r\n\r\n        while (opModeIsActive())\r\n        {\r\n            telemetry.addData(\"Analysis\", pipeline.getAnalysis());\r\n            telemetry.update();\r\n\r\n            // Don't burn CPU cycles busy-looping in this sample\r\n            sleep(50);\r\n        }\r\n    }\r\n\r\n    public static class FreightDeterminationPipeline extends OpenCvPipeline\r\n    {\r\n        /*\r\n         * An enum to define the Freight position\r\n         */\r\n        public enum FreightPosition\r\n        {\r\n            LEFT,\r\n            CENTER,\r\n            RIGHT\r\n        }\r\n\r\n        /*\r\n         * Some color constants\r\n         */\r\n        static final Scalar BLUE = new Scalar(0, 0, 255);\r\n        static final Scalar GREEN = new Scalar(0, 255, 0);\r\n\r\n        /*\r\n         * The core values which define the location and size of the sample regions\r\n         */\r\n        static final Point REGION1_TOPLEFT_ANCHOR_POINT = new Point(109,98);\r\n        static final Point REGION2_TOPLEFT_ANCHOR_POINT = new Point(181,98);\r\n        static final Point REGION3_TOPLEFT_ANCHOR_POINT = new Point(253,98);\r\n        static final int REGION_WIDTH = 20;\r\n        static final int REGION_HEIGHT = 20;\r\n\r\n        /*\r\n         * Points which actually define the sample region rectangles, derived from above values\r\n         *\r\n         * Example of how points A and B work to define a rectangle\r\n         *\r\n         *   ------------------------------------\r\n         *   | (0,0) Point A                    |\r\n         *   |                                  |\r\n         *   |                                  |\r\n         *   |                                  |\r\n         *   |                                  |\r\n         *   |                                  |\r\n         *   |                                  |\r\n         *   |                  Point B (70,50) |\r\n         *   ------------------------------------\r\n         *\r\n         */\r\n        Point region1_pointA = new Point(\r\n                REGION1_TOPLEFT_ANCHOR_POINT.x,\r\n                REGION1_TOPLEFT_ANCHOR_POINT.y);\r\n        Point region1_pointB = new Point(\r\n                REGION1_TOPLEFT_ANCHOR_POINT.x + REGION_WIDTH,\r\n                REGION1_TOPLEFT_ANCHOR_POINT.y + REGION_HEIGHT);\r\n        Point region2_pointA = new Point(\r\n                REGION2_TOPLEFT_ANCHOR_POINT.x,\r\n                REGION2_TOPLEFT_ANCHOR_POINT.y);\r\n        Point region2_pointB = new Point(\r\n                REGION2_TOPLEFT_ANCHOR_POINT.x + REGION_WIDTH,\r\n                REGION2_TOPLEFT_ANCHOR_POINT.y + REGION_HEIGHT);\r\n        Point region3_pointA = new Point(\r\n                REGION3_TOPLEFT_ANCHOR_POINT.x,\r\n                REGION3_TOPLEFT_ANCHOR_POINT.y);\r\n        Point region3_pointB = new Point(\r\n                REGION3_TOPLEFT_ANCHOR_POINT.x + REGION_WIDTH,\r\n                REGION3_TOPLEFT_ANCHOR_POINT.y + REGION_HEIGHT);\r\n\r\n        /*\r\n         * Working variables\r\n         */\r\n        Mat region1_Cb, region2_Cb, region3_Cb;\r\n        Mat YCrCb = new Mat();\r\n        Mat Cb = new Mat();\r\n        int avg1, avg2, avg3;\r\n\r\n        // Volatile since accessed by OpMode thread w/o synchronization\r\n        private volatile FreightPosition position = FreightPosition.LEFT;\r\n\r\n        /*\r\n         * This function takes the RGB frame, converts to YCrCb,\r\n         * and extracts the Cb channel to the 'Cb' variable\r\n         */\r\n        void inputToCb(Mat input)\r\n        {\r\n            Imgproc.cvtColor(input, YCrCb, Imgproc.COLOR_RGB2YCrCb);\r\n            Core.extractChannel(YCrCb, Cb, 2);\r\n        }\r\n\r\n        @Override\r\n        public void init(Mat firstFrame)\r\n        {\r\n            /*\r\n             * We need to call this in order to make sure the 'Cb'\r\n             * object is initialized, so that the submats we make\r\n             * will still be linked to it on subsequent frames. (If\r\n             * the object were to only be initialized in processFrame,\r\n             * then the submats would become delinked because the backing\r\n             * buffer would be re-allocated the first time a real frame\r\n             * was crunched)\r\n             */\r\n            inputToCb(firstFrame);\r\n\r\n            /*\r\n             * Submats are a persistent reference to a region of the parent\r\n             * buffer. Any changes to the child affect the parent, and the\r\n             * reverse also holds true.\r\n             */\r\n            region1_Cb = Cb.submat(new Rect(region1_pointA, region1_pointB));\r\n            region2_Cb = Cb.submat(new Rect(region2_pointA, region2_pointB));\r\n            region3_Cb = Cb.submat(new Rect(region3_pointA, region3_pointB));\r\n        }\r\n\r\n        @Override\r\n        public Mat processFrame(Mat input)\r\n        {\r\n            /*\r\n             * Overview of what we're doing:\r\n             *\r\n             * We first convert to YCrCb color space, from RGB color space.\r\n             * Why do we do this? Well, in the RGB color space, chroma and\r\n             * luma are intertwined. In YCrCb, chroma and luma are separated.\r\n             * YCrCb is a 3-channel color space, just like RGB. YCrCb's 3 channels\r\n             * are Y, the luma channel (which essentially just a B&W image), the\r\n             * Cr channel, which records the difference from red, and the Cb channel,\r\n             * which records the difference from blue. Because chroma and luma are\r\n             * not related in YCrCb, vision code written to look for certain values\r\n             * in the Cr/Cb channels will not be severely affected by differing\r\n             * light intensity, since that difference would most likely just be\r\n             * reflected in the Y channel.\r\n             *\r\n             * After we've converted to YCrCb, we extract just the 2nd channel, the\r\n             * Cb channel. We do this because stones are bright yellow and contrast\r\n             * STRONGLY on the Cb channel against everything else, including Freights\r\n             * (because Freights have a black label).\r\n             *\r\n             * We then take the average pixel value of 3 different regions on that Cb\r\n             * channel, one positioned over each stone. The brightest of the 3 regions\r\n             * is where we assume the Freight to be, since the normal stones show up\r\n             * extremely darkly.\r\n             *\r\n             * We also draw rectangles on the screen showing where the sample regions\r\n             * are, as well as drawing a solid rectangle over top the sample region\r\n             * we believe is on top of the Freight.\r\n             *\r\n             * In order for this whole process to work correctly, each sample region\r\n             * should be positioned in the center of each of the first 3 stones, and\r\n             * be small enough such that only the stone is sampled, and not any of the\r\n             * surroundings.\r\n             */\r\n\r\n            /*\r\n             * Get the Cb channel of the input frame after conversion to YCrCb\r\n             */\r\n            inputToCb(input);\r\n\r\n            /*\r\n             * Compute the average pixel value of each submat region. We're\r\n             * taking the average of a single channel buffer, so the value\r\n             * we need is at index 0. We could have also taken the average\r\n             * pixel value of the 3-channel image, and referenced the value\r\n             * at index 2 here.\r\n             */\r\n            avg1 = (int) Core.mean(region1_Cb).val[0];\r\n            avg2 = (int) Core.mean(region2_Cb).val[0];\r\n            avg3 = (int) Core.mean(region3_Cb).val[0];\r\n\r\n            /*\r\n             * Draw a rectangle showing sample region 1 on the screen.\r\n             * Simply a visual aid. Serves no functional purpose.\r\n             */\r\n            Imgproc.rectangle(\r\n                    input, // Buffer to draw on\r\n                    region1_pointA, // First point which defines the rectangle\r\n                    region1_pointB, // Second point which defines the rectangle\r\n                    BLUE, // The color the rectangle is drawn in\r\n                    2); // Thickness of the rectangle lines\r\n\r\n            /*\r\n             * Draw a rectangle showing sample region 2 on the screen.\r\n             * Simply a visual aid. Serves no functional purpose.\r\n             */\r\n            Imgproc.rectangle(\r\n                    input, // Buffer to draw on\r\n                    region2_pointA, // First point which defines the rectangle\r\n                    region2_pointB, // Second point which defines the rectangle\r\n                    BLUE, // The color the rectangle is drawn in\r\n                    2); // Thickness of the rectangle lines\r\n\r\n            /*\r\n             * Draw a rectangle showing sample region 3 on the screen.\r\n             * Simply a visual aid. Serves no functional purpose.\r\n             */\r\n            Imgproc.rectangle(\r\n                    input, // Buffer to draw on\r\n                    region3_pointA, // First point which defines the rectangle\r\n                    region3_pointB, // Second point which defines the rectangle\r\n                    BLUE, // The color the rectangle is drawn in\r\n                    2); // Thickness of the rectangle lines\r\n\r\n\r\n            /*\r\n             * Find the max of the 3 averages\r\n             */\r\n            int maxOneTwo = Math.max(avg1, avg2);\r\n            int max = Math.max(maxOneTwo, avg3);\r\n\r\n            /*\r\n             * Now that we found the max, we actually need to go and\r\n             * figure out which sample region that value was from\r\n             */\r\n            if(max == avg1) // Was it from region 1?\r\n            {\r\n                position = FreightPosition.LEFT; // Record our analysis\r\n\r\n                /*\r\n                 * Draw a solid rectangle on top of the chosen region.\r\n                 * Simply a visual aid. Serves no functional purpose.\r\n                 */\r\n                Imgproc.rectangle(\r\n                        input, // Buffer to draw on\r\n                        region1_pointA, // First point which defines the rectangle\r\n                        region1_pointB, // Second point which defines the rectangle\r\n                        GREEN, // The color the rectangle is drawn in\r\n                        -1); // Negative thickness means solid fill\r\n            }\r\n            else if(max == avg2) // Was it from region 2?\r\n            {\r\n                position = FreightPosition.CENTER; // Record our analysis\r\n\r\n                /*\r\n                 * Draw a solid rectangle on top of the chosen region.\r\n                 * Simply a visual aid. Serves no functional purpose.\r\n                 */\r\n                Imgproc.rectangle(\r\n                        input, // Buffer to draw on\r\n                        region2_pointA, // First point which defines the rectangle\r\n                        region2_pointB, // Second point which defines the rectangle\r\n                        GREEN, // The color the rectangle is drawn in\r\n                        -1); // Negative thickness means solid fill\r\n            }\r\n            else if(max == avg3) // Was it from region 3?\r\n            {\r\n                position = FreightPosition.RIGHT; // Record our analysis\r\n\r\n                /*\r\n                 * Draw a solid rectangle on top of the chosen region.\r\n                 * Simply a visual aid. Serves no functional purpose.\r\n                 */\r\n                Imgproc.rectangle(\r\n                        input, // Buffer to draw on\r\n                        region3_pointA, // First point which defines the rectangle\r\n                        region3_pointB, // Second point which defines the rectangle\r\n                        GREEN, // The color the rectangle is drawn in\r\n                        -1); // Negative thickness means solid fill\r\n            }\r\n\r\n            /*\r\n             * Render the 'input' buffer to the viewport. But note this is not\r\n             * simply rendering the raw camera feed, because we called functions\r\n             * to add some annotations to this buffer earlier up.\r\n             */\r\n            return input;\r\n        }\r\n\r\n        /*\r\n         * Call this from the OpMode thread to obtain the latest analysis\r\n         */\r\n        public FreightPosition getAnalysis()\r\n        {\r\n            return position;\r\n        }\r\n    }\r\n}\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/TeamCode/src/main/java/org/camera/openftc/easyopencv/FreightDeterminationExample.java b/TeamCode/src/main/java/org/camera/openftc/easyopencv/FreightDeterminationExample.java
--- a/TeamCode/src/main/java/org/camera/openftc/easyopencv/FreightDeterminationExample.java	(revision e88ab475f7abf8372d216d278ad86bc2d6146a80)
+++ b/TeamCode/src/main/java/org/camera/openftc/easyopencv/FreightDeterminationExample.java	(date 1635107914656)
@@ -21,11 +21,20 @@
 
 package org.camera.openftc.easyopencv;
 
+import com.acmerobotics.dashboard.FtcDashboard;
+import com.acmerobotics.dashboard.config.Config;
+import com.acmerobotics.dashboard.telemetry.TelemetryPacket;
 import com.qualcomm.robotcore.eventloop.opmode.Disabled;
 import com.qualcomm.robotcore.eventloop.opmode.LinearOpMode;
 import com.qualcomm.robotcore.eventloop.opmode.TeleOp;
 
+import org.firstinspires.ftc.robotcore.external.ClassFactory;
 import org.firstinspires.ftc.robotcore.external.hardware.camera.WebcamName;
+import org.firstinspires.ftc.robotcore.external.navigation.VuforiaLocalizer;
+import org.firstinspires.ftc.teamcode.R;
+import org.ftc.waterloo.h2oloobots.AttachmentControl;
+import org.ftc.waterloo.h2oloobots.DriveTrain;
+import org.ftc.waterloo.h2oloobots.TelemetryControl;
 import org.opencv.core.Core;
 import org.opencv.core.Mat;
 import org.opencv.core.Point;
@@ -44,15 +53,29 @@
  * the sample regions over the first 3 stones.
  */
 @TeleOp
-@Disabled
+@Config
+//@Disabled
 public class FreightDeterminationExample extends LinearOpMode
 {
+
     OpenCvWebcam webcam;
     FreightDeterminationPipeline pipeline;
+    public static final String VUFORIA_LICENSE_KEY = "AUhZBUP/////AAABmYEGpdLRPksVnc0ztTr0AVMWkvz/IqsD3cuBMKME0ZRQfnHZVGjZvnw138iHecuD+jNRvjNyidYb2ZgXwzaSru+n6xtkfyQvN7GU2s/kXkxMtJm5EGwMUkDqULQCEnqtm68Cc0FfKCV+aygL1qRRMHwfttGd82y5GqqnaEejg9Ummb/e7tGIaHsSlQJ9Met3Wwo9CzXCMZUa+SOq2orh0b2dv0Gj0xi4vzjBKdllxE6aXRYgXfq2h7Nxnx3MrdgnyUTn5FEJicPbXU4knlZEXE2+qSSmMeCaXw4KzSF/e5nDilQYgTYxRqE06Qzu1t0xqZQsIHnAdkFjmEdLpFwePjthqUUl2mRr7jGCNqZgmH1u";
+
+    public DriveTrain driveTrain = new DriveTrain();
+    public AttachmentControl attachmentControl = new AttachmentControl();
+    public TelemetryControl telemetryControl = new TelemetryControl();
 
     @Override
     public void runOpMode()
     {
+
+        FtcDashboard dashboard = FtcDashboard.getInstance();
+        TelemetryPacket packet = new TelemetryPacket();
+
+        driveTrain.FourMotorInit(false, hardwareMap);
+//        attachmentControl.attachmentInit(hardwareMap, telemetry, 1);
+
         /**
          * NOTE: Many comments have been omitted from this sample for the
          * sake of conciseness. If you're just starting out with EasyOpenCv,
@@ -69,6 +92,15 @@
         // out when the RC activity is in portrait. We do our actual image processing assuming
         // landscape orientation, though.
 
+        msStuckDetectStop = 2500;
+
+        VuforiaLocalizer.Parameters vuforiaParams = new VuforiaLocalizer.Parameters(org.firstinspires.ftc.teamcode.R.id.cameraMonitorViewId);
+        vuforiaParams.vuforiaLicenseKey = VUFORIA_LICENSE_KEY;
+        vuforiaParams.cameraDirection = VuforiaLocalizer.CameraDirection.BACK;
+        VuforiaLocalizer vuforia = ClassFactory.getInstance().createVuforia(vuforiaParams);
+
+        FtcDashboard.getInstance().startCameraStream(webcam, 24);
+
         webcam.setMillisecondsPermissionTimeout(2500);
         webcam.openCameraDeviceAsync(new OpenCvCamera.AsyncCameraOpenListener()
         {
@@ -91,11 +123,25 @@
 
         while (opModeIsActive())
         {
+
+            FtcDashboard.getInstance().startCameraStream(webcam, 24);
+
+            driveTrain.MecanumTeleOp(gamepad1.left_stick_y, gamepad1.left_stick_x, gamepad1.right_stick_x, false, telemetry);
+//            attachmentControl.duckMotorTeleop(gamepad1.x);
+//            attachmentControl.liftMotorMove(gamepad1.y, gamepad1.a);
+
             telemetry.addData("Analysis", pipeline.getAnalysis());
+            telemetry.addData("Front Left Motor Power", driveTrain.fl.getPower());
+            telemetry.addData("Front Right Motor Power", driveTrain.fr.getPower());
+            telemetry.addData("Back Left Motor Power", driveTrain.bl.getPower());
+            telemetry.addData("Back Right Motor Power", driveTrain.br.getPower());
+//            telemetry.addData("Lift Motor Position", attachmentControl.LiftMotor.getCurrentPosition());
+//            telemetry.addData("Duck Motor Power", attachmentControl.DuckMotor.getPower());
             telemetry.update();
 
+            packet.put("Analysis", pipeline.getAnalysis());
+
             // Don't burn CPU cycles busy-looping in this sample
-            sleep(50);
         }
     }
 
Index: FtcRobotController/build/generated/source/buildConfig/debug/com/qualcomm/ftcrobotcontroller/BuildConfig.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>/**\r\n * Automatically generated file. DO NOT MODIFY\r\n */\r\npackage com.qualcomm.ftcrobotcontroller;\r\n\r\npublic final class BuildConfig {\r\n  public static final boolean DEBUG = Boolean.parseBoolean(\"true\");\r\n  public static final String LIBRARY_PACKAGE_NAME = \"com.qualcomm.ftcrobotcontroller\";\r\n  public static final String BUILD_TYPE = \"debug\";\r\n  // Field from default config.\r\n  public static final String APP_BUILD_TIME = \"2021-10-21T19:41:09.005-0500\";\r\n}\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/FtcRobotController/build/generated/source/buildConfig/debug/com/qualcomm/ftcrobotcontroller/BuildConfig.java b/FtcRobotController/build/generated/source/buildConfig/debug/com/qualcomm/ftcrobotcontroller/BuildConfig.java
--- a/FtcRobotController/build/generated/source/buildConfig/debug/com/qualcomm/ftcrobotcontroller/BuildConfig.java	(revision e88ab475f7abf8372d216d278ad86bc2d6146a80)
+++ b/FtcRobotController/build/generated/source/buildConfig/debug/com/qualcomm/ftcrobotcontroller/BuildConfig.java	(date 1635108798685)
@@ -8,5 +8,5 @@
   public static final String LIBRARY_PACKAGE_NAME = "com.qualcomm.ftcrobotcontroller";
   public static final String BUILD_TYPE = "debug";
   // Field from default config.
-  public static final String APP_BUILD_TIME = "2021-10-21T19:41:09.005-0500";
+  public static final String APP_BUILD_TIME = "2021-10-24T15:53:18.460-0500";
 }
Index: TeamCode/build/tmp/compileDebugJavaWithJavac/source-classes-mapping.txt
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>org/camera/openftc/easyopencv/WebcamExample.java\r\n org.camera.openftc.easyopencv.WebcamExample\r\n org.camera.openftc.easyopencv.WebcamExample$1\r\n org.camera.openftc.easyopencv.WebcamExample$SamplePipeline\r\norg/camera/openftc/easyopencv/TimestampedPipelineExample.java\r\n org.camera.openftc.easyopencv.TimestampedPipelineExample\r\n org.camera.openftc.easyopencv.TimestampedPipelineExample$1\r\n org.camera.openftc.easyopencv.TimestampedPipelineExample$SampleTimestampedPipeline\r\ncom/kauailabs/navx/SensorNavXRawOp.java\r\n com.kauailabs.navx.SensorNavXRawOp\r\norg/firstinspires/ftc/teamcode/TestTeleop.java\r\n org.firstinspires.ftc.teamcode.TestTeleop\r\ncom/kauailabs/navx/ConceptNavXDriveAutoBalance.java\r\n com.kauailabs.navx.ConceptNavXDriveAutoBalance\r\norg/firstinspires/ftc/teamcode/Tele1.java\r\n org.firstinspires.ftc.teamcode.Tele1\r\norg/firstinspires/ftc/teamcode/CameraStreamTest.java\r\n org.firstinspires.ftc.teamcode.CameraStreamTest\r\norg/camera/openftc/easyopencv/TrackerApiExample.java\r\n org.camera.openftc.easyopencv.TrackerApiExample\r\n org.camera.openftc.easyopencv.TrackerApiExample$1\r\n org.camera.openftc.easyopencv.TrackerApiExample$UselessColorBoxDrawingTracker\r\norg/camera/openftc/easyopencv/SwitchableWebcamExample.java\r\n org.camera.openftc.easyopencv.SwitchableWebcamExample\r\n org.camera.openftc.easyopencv.SwitchableWebcamExample$1\r\n org.camera.openftc.easyopencv.SwitchableWebcamExample$SamplePipeline\r\norg/firstinspires/ftc/teamcode/Auto1.java\r\n org.firstinspires.ftc.teamcode.Auto1\r\norg/camera/openftc/easyopencv/FreightDeterminationExample.java\r\n org.camera.openftc.easyopencv.FreightDeterminationExample\r\n org.camera.openftc.easyopencv.FreightDeterminationExample$1\r\n org.camera.openftc.easyopencv.FreightDeterminationExample$FreightDeterminationPipeline\r\n org.camera.openftc.easyopencv.FreightDeterminationExample$FreightDeterminationPipeline$FreightPosition\r\ncom/kauailabs/navx/ConceptNavXDriveStraightPIDLinearOp.java\r\n com.kauailabs.navx.ConceptNavXDriveStraightPIDLinearOp\r\ncom/kauailabs/navx/ConceptNavXCollisionDetectionOp.java\r\n com.kauailabs.navx.ConceptNavXCollisionDetectionOp\r\ncom/kauailabs/navx/ConceptNavXMotionDetectionOp.java\r\n com.kauailabs.navx.ConceptNavXMotionDetectionOp\r\norg/ftc/waterloo/h2oloobots/TelemetryControl.java\r\n org.ftc.waterloo.h2oloobots.TelemetryControl\r\norg/ftc/waterloo/h2oloobots/DriveTrain.java\r\n org.ftc.waterloo.h2oloobots.DriveTrain\r\norg/camera/openftc/easyopencv/StoneOrientationExample.java\r\n org.camera.openftc.easyopencv.StoneOrientationExample\r\n org.camera.openftc.easyopencv.StoneOrientationExample$1\r\n org.camera.openftc.easyopencv.StoneOrientationExample$2\r\n org.camera.openftc.easyopencv.StoneOrientationExample$StoneOrientationAnalysisPipeline\r\n org.camera.openftc.easyopencv.StoneOrientationExample$StoneOrientationAnalysisPipeline$AnalyzedStone\r\n org.camera.openftc.easyopencv.StoneOrientationExample$StoneOrientationAnalysisPipeline$ContourRegionAnalysis\r\n org.camera.openftc.easyopencv.StoneOrientationExample$StoneOrientationAnalysisPipeline$Stage\r\n org.camera.openftc.easyopencv.StoneOrientationExample$StoneOrientationAnalysisPipeline$StoneOrientation\r\ncom/kauailabs/navx/SensorNavXProcessedOp.java\r\n com.kauailabs.navx.SensorNavXProcessedOp\r\ncom/kauailabs/navx/ConceptNavXDriveStraightPIDLoopOp.java\r\n com.kauailabs.navx.ConceptNavXDriveStraightPIDLoopOp\r\norg/camera/openftc/easyopencv/PipelineRecordingExample.java\r\n org.camera.openftc.easyopencv.PipelineRecordingExample\r\n org.camera.openftc.easyopencv.PipelineRecordingExample$1\r\n org.camera.openftc.easyopencv.PipelineRecordingExample$SamplePipeline\r\ncom/kauailabs/navx/ConceptNavXPerformanceTuningOp.java\r\n com.kauailabs.navx.ConceptNavXPerformanceTuningOp\r\ncom/kauailabs/navx/ConceptNavXRotateToAnglePIDLoopOp.java\r\n com.kauailabs.navx.ConceptNavXRotateToAnglePIDLoopOp\r\norg/camera/openftc/easyopencv/OpenCvAndVuforiaOnSameCameraExample.java\r\n org.camera.openftc.easyopencv.OpenCvAndVuforiaOnSameCameraExample\r\n org.camera.openftc.easyopencv.OpenCvAndVuforiaOnSameCameraExample$1\r\n org.camera.openftc.easyopencv.OpenCvAndVuforiaOnSameCameraExample$UselessColorBoxDrawingPipeline\r\norg/ftc/waterloo/h2oloobots/AttachmentControl.java\r\n org.ftc.waterloo.h2oloobots.AttachmentControl\r\n org.ftc.waterloo.h2oloobots.AttachmentControl$1\r\n org.ftc.waterloo.h2oloobots.AttachmentControl$LiftMotorPosition\r\norg/firstinspires/ftc/teamcode/BuildConfig.java\r\n org.firstinspires.ftc.teamcode.BuildConfig\r\norg/camera/openftc/easyopencv/PipelineStageSwitchingExample.java\r\n org.camera.openftc.easyopencv.PipelineStageSwitchingExample\r\n org.camera.openftc.easyopencv.PipelineStageSwitchingExample$1\r\n org.camera.openftc.easyopencv.PipelineStageSwitchingExample$2\r\n org.camera.openftc.easyopencv.PipelineStageSwitchingExample$StageSwitchingPipeline\r\n org.camera.openftc.easyopencv.PipelineStageSwitchingExample$StageSwitchingPipeline$Stage\r\ncom/kauailabs/navx/ConceptNavXRotateToAnglePIDLinearOp.java\r\n com.kauailabs.navx.ConceptNavXRotateToAnglePIDLinearOp\r\ncom/kauailabs/navx/ConceptNavXZeroYawOp.java\r\n com.kauailabs.navx.ConceptNavXZeroYawOp\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/TeamCode/build/tmp/compileDebugJavaWithJavac/source-classes-mapping.txt b/TeamCode/build/tmp/compileDebugJavaWithJavac/source-classes-mapping.txt
--- a/TeamCode/build/tmp/compileDebugJavaWithJavac/source-classes-mapping.txt	(revision e88ab475f7abf8372d216d278ad86bc2d6146a80)
+++ b/TeamCode/build/tmp/compileDebugJavaWithJavac/source-classes-mapping.txt	(date 1635108801013)
@@ -80,5 +80,7 @@
  org.camera.openftc.easyopencv.PipelineStageSwitchingExample$StageSwitchingPipeline$Stage
 com/kauailabs/navx/ConceptNavXRotateToAnglePIDLinearOp.java
  com.kauailabs.navx.ConceptNavXRotateToAnglePIDLinearOp
+org/camera/openftc/easyopencv/ConceptTensorFlowObjectDetectionWebcam.java
+ org.camera.openftc.easyopencv.ConceptTensorFlowObjectDetectionWebcam
 com/kauailabs/navx/ConceptNavXZeroYawOp.java
  com.kauailabs.navx.ConceptNavXZeroYawOp
Index: TeamCode/src/main/java/org/camera/openftc/easyopencv/ConceptTensorFlowObjectDetectionWebcam.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/TeamCode/src/main/java/org/camera/openftc/easyopencv/ConceptTensorFlowObjectDetectionWebcam.java b/TeamCode/src/main/java/org/camera/openftc/easyopencv/ConceptTensorFlowObjectDetectionWebcam.java
new file mode 100644
--- /dev/null	(date 1635109806396)
+++ b/TeamCode/src/main/java/org/camera/openftc/easyopencv/ConceptTensorFlowObjectDetectionWebcam.java	(date 1635109806396)
@@ -0,0 +1,209 @@
+/* Copyright (c) 2019 FIRST. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without modification,
+ * are permitted (subject to the limitations in the disclaimer below) provided that
+ * the following conditions are met:
+ *
+ * Redistributions of source code must retain the above copyright notice, this list
+ * of conditions and the following disclaimer.
+ *
+ * Redistributions in binary form must reproduce the above copyright notice, this
+ * list of conditions and the following disclaimer in the documentation and/or
+ * other materials provided with the distribution.
+ *
+ * Neither the name of FIRST nor the names of its contributors may be used to endorse or
+ * promote products derived from this software without specific prior written permission.
+ *
+ * NO EXPRESS OR IMPLIED LICENSES TO ANY PARTY'S PATENT RIGHTS ARE GRANTED BY THIS
+ * LICENSE. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+ * "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,
+ * THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE
+ * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+ * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
+ * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
+ * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
+ * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+package org.camera.openftc.easyopencv;
+
+import com.acmerobotics.dashboard.FtcDashboard;
+import com.qualcomm.robotcore.eventloop.opmode.LinearOpMode;
+import com.qualcomm.robotcore.eventloop.opmode.TeleOp;
+
+import org.firstinspires.ftc.robotcore.external.ClassFactory;
+import org.firstinspires.ftc.robotcore.external.hardware.camera.WebcamName;
+import org.firstinspires.ftc.robotcore.external.navigation.VuforiaLocalizer;
+import org.firstinspires.ftc.robotcore.external.tfod.Recognition;
+import org.firstinspires.ftc.robotcore.external.tfod.TFObjectDetector;
+
+import java.util.List;
+
+/**
+ * This 2020-2021 OpMode illustrates the basics of using the TensorFlow Object Detection API to
+ * determine the position of the Freight Frenzy game elements.
+ *
+ * Use Android Studio to Copy this Class, and Paste it into your team's code folder with a new name.
+ * Remove or comment out the @Disabled line to add this opmode to the Driver Station OpMode list.
+ *
+ * IMPORTANT: In order to use this OpMode, you need to obtain your own Vuforia license key as
+ * is explained below.
+ */
+@TeleOp(name = "Concept: TensorFlow Object Detection Webcam", group = "Concept")
+//@Disabled
+public class ConceptTensorFlowObjectDetectionWebcam extends LinearOpMode {
+  /* Note: This sample uses the all-objects Tensor Flow model (FreightFrenzy_BCDM.tflite), which contains
+   * the following 4 detectable objects
+   *  0: Ball,
+   *  1: Cube,
+   *  2: Duck,
+   *  3: Marker (duck location tape marker)
+   *
+   *  Two additional model assets are available which only contain a subset of the objects:
+   *  FreightFrenzy_BC.tflite  0: Ball,  1: Cube
+   *  FreightFrenzy_DM.tflite  0: Duck,  1: Marker
+   */
+    private static final String TFOD_MODEL_ASSET = "FreightFrenzy_BCDM.tflite";
+    private static final String[] LABELS = {
+      "Ball",
+      "Cube",
+      "Duck",
+      "Marker"
+    };
+
+    /*
+     * IMPORTANT: You need to obtain your own license key to use Vuforia. The string below with which
+     * 'parameters.vuforiaLicenseKey' is initialized is for illustration only, and will not function.
+     * A Vuforia 'Development' license key, can be obtained free of charge from the Vuforia developer
+     * web site at https://developer.vuforia.com/license-manager.
+     *
+     * Vuforia license keys are always 380 characters long, and look as if they contain mostly
+     * random data. As an example, here is a example of a fragment of a valid key:
+     *      ... yIgIzTqZ4mWjk9wd3cZO9T1axEqzuhxoGlfOOI2dRzKS4T0hQ8kT ...
+     * Once you've obtained a license key, copy the string from the Vuforia web site
+     * and paste it in to your code on the next line, between the double quotes.
+     */
+    private static final String VUFORIA_KEY =
+            "AUhZBUP/////AAABmYEGpdLRPksVnc0ztTr0AVMWkvz/IqsD3cuBMKME0ZRQfnHZVGjZvnw138iHecuD+jNRvjNyidYb2ZgXwzaSru+n6xtkfyQvN7GU2s/kXkxMtJm5EGwMUkDqULQCEnqtm68Cc0FfKCV+aygL1qRRMHwfttGd82y5GqqnaEejg9Ummb/e7tGIaHsSlQJ9Met3Wwo9CzXCMZUa+SOq2orh0b2dv0Gj0xi4vzjBKdllxE6aXRYgXfq2h7Nxnx3MrdgnyUTn5FEJicPbXU4knlZEXE2+qSSmMeCaXw4KzSF/e5nDilQYgTYxRqE06Qzu1t0xqZQsIHnAdkFjmEdLpFwePjthqUUl2mRr7jGCNqZgmH1u";
+
+    /**
+     * {@link #vuforia} is the variable we will use to store our instance of the Vuforia
+     * localization engine.
+     */
+    private VuforiaLocalizer vuforia;
+
+    /**
+     * {@link #tfod} is the variable we will use to store our instance of the TensorFlow Object
+     * Detection engine.
+     */
+    private TFObjectDetector tfod;
+
+    @Override
+    public void runOpMode() {
+        // The TFObjectDetector uses the camera frames from the VuforiaLocalizer, so we create that
+        // first.
+        initVuforia();
+        initTfod();
+
+        /**
+         * Activate TensorFlow Object Detection before we wait for the start command.
+         * Do it here so that the Camera Stream window will have the TensorFlow annotations visible.
+         **/
+        if (tfod != null) {
+            tfod.activate();
+
+            // The TensorFlow software will scale the input images from the camera to a lower resolution.
+            // This can result in lower detection accuracy at longer distances (> 55cm or 22").
+            // If your target is at distance greater than 50 cm (20") you can adjust the magnification value
+            // to artificially zoom in to the center of image.  For best results, the "aspectRatio" argument
+            // should be set to the value of the images used to create the TensorFlow Object Detection model
+            // (typically 16/9).
+            tfod.setZoom(2.5, 16.0/9.0);
+        }
+
+        /** Wait for the game to begin */
+        telemetry.addData(">", "Press Play to start op mode");
+        telemetry.update();
+
+        FtcDashboard.getInstance().startCameraStream(vuforia, 24);
+
+        waitForStart();
+
+        if (opModeIsActive()) {
+
+            while (opModeIsActive()) {
+
+                FtcDashboard.getInstance().startCameraStream(vuforia, 24);
+
+                if (tfod != null) {
+
+                    // getUpdatedRecognitions() will return null if no new information is available since
+                    // the last time that call was made.
+
+                    List<Recognition> updatedRecognitions = tfod.getUpdatedRecognitions();
+
+                    if (updatedRecognitions != null) {
+
+                      telemetry.addData("# Object Detected", updatedRecognitions.size());
+
+                      // step through the list of recognitions and display boundary info.
+
+                      int i = 0;
+
+                      for (Recognition recognition : updatedRecognitions) {
+
+                        telemetry.addData(String.format("label (%d)", i), recognition.getLabel());
+                        telemetry.addData(String.format("  left,top (%d)", i), "%.03f , %.03f",
+                                recognition.getLeft(), recognition.getTop());
+                        telemetry.addData(String.format("  right,bottom (%d)", i), "%.03f , %.03f",
+                                recognition.getRight(), recognition.getBottom());
+                        i++;
+
+                      }
+
+                      telemetry.update();
+
+                    }
+
+                }
+
+            }
+
+        }
+
+    }
+
+    /**
+     * Initialize the Vuforia localization engine.
+     */
+    private void initVuforia() {
+        /*
+         * Configure Vuforia by creating a Parameter object, and passing it to the Vuforia engine.
+         */
+        VuforiaLocalizer.Parameters parameters = new VuforiaLocalizer.Parameters();
+
+        parameters.vuforiaLicenseKey = VUFORIA_KEY;
+        parameters.cameraName = hardwareMap.get(WebcamName.class, "Webcam 1");
+
+        //  Instantiate the Vuforia engine
+        vuforia = ClassFactory.getInstance().createVuforia(parameters);
+
+        // Loading trackables is not necessary for the TensorFlow Object Detection engine.
+    }
+
+    /**
+     * Initialize the TensorFlow Object Detection engine.
+     */
+    private void initTfod() {
+        int tfodMonitorViewId = hardwareMap.appContext.getResources().getIdentifier(
+            "tfodMonitorViewId", "id", hardwareMap.appContext.getPackageName());
+        TFObjectDetector.Parameters tfodParameters = new TFObjectDetector.Parameters(tfodMonitorViewId);
+       tfodParameters.minResultConfidence = 0.8f;
+       tfodParameters.isModelTensorFlow2 = true;
+       tfodParameters.inputSize = 320;
+       tfod = ClassFactory.getInstance().createTFObjectDetector(tfodParameters, vuforia);
+       tfod.loadModelFromAsset(TFOD_MODEL_ASSET, LABELS);
+    }
+}
Index: TeamCode/build/intermediates/incremental/packageDebug/tmp/debug/dex-renamer-state.txt
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>#Thu Oct 21 19:41:11 CDT 2021\r\npath.4=0/classes.dex\r\npath.3=8/classes.dex\r\npath.2=11/classes.dex\r\nrenamed.9=classes10.dex\r\npath.1=10/classes.dex\r\nrenamed.8=classes9.dex\r\npath.8=4/classes.dex\r\npath.7=15/classes.dex\r\npath.6=14/classes.dex\r\npath.5=12/classes.dex\r\npath.0=classes.dex\r\nbase.4=C\\:\\\\Users\\\\frc5837\\\\Desktop\\\\FreightFrenzyV1\\\\TeamCode\\\\build\\\\intermediates\\\\dex\\\\debug\\\\mergeProjectDexDebug\\\\0\\\\classes.dex\r\nbase.3=C\\:\\\\Users\\\\frc5837\\\\Desktop\\\\FreightFrenzyV1\\\\TeamCode\\\\build\\\\intermediates\\\\dex\\\\debug\\\\mergeLibDexDebug\\\\8\\\\classes.dex\r\nbase.2=C\\:\\\\Users\\\\frc5837\\\\Desktop\\\\FreightFrenzyV1\\\\TeamCode\\\\build\\\\intermediates\\\\dex\\\\debug\\\\mergeLibDexDebug\\\\11\\\\classes.dex\r\nbase.1=C\\:\\\\Users\\\\frc5837\\\\Desktop\\\\FreightFrenzyV1\\\\TeamCode\\\\build\\\\intermediates\\\\dex\\\\debug\\\\mergeLibDexDebug\\\\10\\\\classes.dex\r\nbase.0=C\\:\\\\Users\\\\frc5837\\\\Desktop\\\\FreightFrenzyV1\\\\TeamCode\\\\build\\\\intermediates\\\\dex\\\\debug\\\\mergeExtDexDebug\\\\classes.dex\r\nrenamed.3=classes4.dex\r\nrenamed.2=classes3.dex\r\nrenamed.1=classes2.dex\r\nrenamed.0=classes.dex\r\nbase.9=C\\:\\\\Users\\\\frc5837\\\\Desktop\\\\FreightFrenzyV1\\\\TeamCode\\\\build\\\\intermediates\\\\dex\\\\debug\\\\mergeExtDexDebug\\\\classes2.dex\r\npath.9=classes2.dex\r\nrenamed.7=classes8.dex\r\nbase.8=C\\:\\\\Users\\\\frc5837\\\\Desktop\\\\FreightFrenzyV1\\\\TeamCode\\\\build\\\\intermediates\\\\dex\\\\debug\\\\mergeProjectDexDebug\\\\4\\\\classes.dex\r\nrenamed.6=classes7.dex\r\nbase.7=C\\:\\\\Users\\\\frc5837\\\\Desktop\\\\FreightFrenzyV1\\\\TeamCode\\\\build\\\\intermediates\\\\dex\\\\debug\\\\mergeProjectDexDebug\\\\15\\\\classes.dex\r\nrenamed.5=classes6.dex\r\nbase.6=C\\:\\\\Users\\\\frc5837\\\\Desktop\\\\FreightFrenzyV1\\\\TeamCode\\\\build\\\\intermediates\\\\dex\\\\debug\\\\mergeProjectDexDebug\\\\14\\\\classes.dex\r\nrenamed.4=classes5.dex\r\nbase.5=C\\:\\\\Users\\\\frc5837\\\\Desktop\\\\FreightFrenzyV1\\\\TeamCode\\\\build\\\\intermediates\\\\dex\\\\debug\\\\mergeProjectDexDebug\\\\12\\\\classes.dex\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/TeamCode/build/intermediates/incremental/packageDebug/tmp/debug/dex-renamer-state.txt b/TeamCode/build/intermediates/incremental/packageDebug/tmp/debug/dex-renamer-state.txt
--- a/TeamCode/build/intermediates/incremental/packageDebug/tmp/debug/dex-renamer-state.txt	(revision e88ab475f7abf8372d216d278ad86bc2d6146a80)
+++ b/TeamCode/build/intermediates/incremental/packageDebug/tmp/debug/dex-renamer-state.txt	(date 1635108801357)
@@ -1,4 +1,4 @@
-#Thu Oct 21 19:41:11 CDT 2021
+#Sun Oct 24 15:53:21 CDT 2021
 path.4=0/classes.dex
 path.3=8/classes.dex
 path.2=11/classes.dex
Index: .idea/deploymentTargetDropDown.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/.idea/deploymentTargetDropDown.xml b/.idea/deploymentTargetDropDown.xml
new file mode 100644
--- /dev/null	(date 1635103554777)
+++ b/.idea/deploymentTargetDropDown.xml	(date 1635103554777)
@@ -0,0 +1,17 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<project version="4">
+  <component name="deploymentTargetDropDown">
+    <runningDeviceTargetSelectedWithDropDown>
+      <Target>
+        <type value="RUNNING_DEVICE_TARGET" />
+        <deviceKey>
+          <Key>
+            <type value="SERIAL_NUMBER" />
+            <value value="192.168.43.1:5555" />
+          </Key>
+        </deviceKey>
+      </Target>
+    </runningDeviceTargetSelectedWithDropDown>
+    <timeTargetWasSelectedWithDropDown value="2021-10-24T19:25:44.810616Z" />
+  </component>
+</project>
\ No newline at end of file
Index: FtcRobotController/src/main/java/org/firstinspires/ftc/robotcontroller/external/samples/ConceptTensorFlowObjectDetectionWebcam.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>/* Copyright (c) 2019 FIRST. All rights reserved.\r\n *\r\n * Redistribution and use in source and binary forms, with or without modification,\r\n * are permitted (subject to the limitations in the disclaimer below) provided that\r\n * the following conditions are met:\r\n *\r\n * Redistributions of source code must retain the above copyright notice, this list\r\n * of conditions and the following disclaimer.\r\n *\r\n * Redistributions in binary form must reproduce the above copyright notice, this\r\n * list of conditions and the following disclaimer in the documentation and/or\r\n * other materials provided with the distribution.\r\n *\r\n * Neither the name of FIRST nor the names of its contributors may be used to endorse or\r\n * promote products derived from this software without specific prior written permission.\r\n *\r\n * NO EXPRESS OR IMPLIED LICENSES TO ANY PARTY'S PATENT RIGHTS ARE GRANTED BY THIS\r\n * LICENSE. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\r\n * \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\r\n * THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\r\n * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE\r\n * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\r\n * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\r\n * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\r\n * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\r\n * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\r\n * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\r\n */\r\n\r\npackage org.firstinspires.ftc.robotcontroller.external.samples;\r\n\r\nimport com.qualcomm.robotcore.eventloop.opmode.Disabled;\r\nimport com.qualcomm.robotcore.eventloop.opmode.LinearOpMode;\r\nimport com.qualcomm.robotcore.eventloop.opmode.TeleOp;\r\nimport java.util.List;\r\nimport org.firstinspires.ftc.robotcore.external.ClassFactory;\r\nimport org.firstinspires.ftc.robotcore.external.hardware.camera.WebcamName;\r\nimport org.firstinspires.ftc.robotcore.external.navigation.VuforiaLocalizer;\r\nimport org.firstinspires.ftc.robotcore.external.tfod.TFObjectDetector;\r\nimport org.firstinspires.ftc.robotcore.external.tfod.Recognition;\r\n\r\n/**\r\n * This 2020-2021 OpMode illustrates the basics of using the TensorFlow Object Detection API to\r\n * determine the position of the Freight Frenzy game elements.\r\n *\r\n * Use Android Studio to Copy this Class, and Paste it into your team's code folder with a new name.\r\n * Remove or comment out the @Disabled line to add this opmode to the Driver Station OpMode list.\r\n *\r\n * IMPORTANT: In order to use this OpMode, you need to obtain your own Vuforia license key as\r\n * is explained below.\r\n */\r\n@TeleOp(name = \"Concept: TensorFlow Object Detection Webcam\", group = \"Concept\")\r\n//@Disabled\r\npublic class ConceptTensorFlowObjectDetectionWebcam extends LinearOpMode {\r\n  /* Note: This sample uses the all-objects Tensor Flow model (FreightFrenzy_BCDM.tflite), which contains\r\n   * the following 4 detectable objects\r\n   *  0: Ball,\r\n   *  1: Cube,\r\n   *  2: Duck,\r\n   *  3: Marker (duck location tape marker)\r\n   *\r\n   *  Two additional model assets are available which only contain a subset of the objects:\r\n   *  FreightFrenzy_BC.tflite  0: Ball,  1: Cube\r\n   *  FreightFrenzy_DM.tflite  0: Duck,  1: Marker\r\n   */\r\n    private static final String TFOD_MODEL_ASSET = \"FreightFrenzy_BCDM.tflite\";\r\n    private static final String[] LABELS = {\r\n      \"Ball\",\r\n      \"Cube\",\r\n      \"Duck\",\r\n      \"Marker\"\r\n    };\r\n\r\n    /*\r\n     * IMPORTANT: You need to obtain your own license key to use Vuforia. The string below with which\r\n     * 'parameters.vuforiaLicenseKey' is initialized is for illustration only, and will not function.\r\n     * A Vuforia 'Development' license key, can be obtained free of charge from the Vuforia developer\r\n     * web site at https://developer.vuforia.com/license-manager.\r\n     *\r\n     * Vuforia license keys are always 380 characters long, and look as if they contain mostly\r\n     * random data. As an example, here is a example of a fragment of a valid key:\r\n     *      ... yIgIzTqZ4mWjk9wd3cZO9T1axEqzuhxoGlfOOI2dRzKS4T0hQ8kT ...\r\n     * Once you've obtained a license key, copy the string from the Vuforia web site\r\n     * and paste it in to your code on the next line, between the double quotes.\r\n     */\r\n    private static final String VUFORIA_KEY =\r\n            \"AUhZBUP/////AAABmYEGpdLRPksVnc0ztTr0AVMWkvz/IqsD3cuBMKME0ZRQfnHZVGjZvnw138iHecuD+jNRvjNyidYb2ZgXwzaSru+n6xtkfyQvN7GU2s/kXkxMtJm5EGwMUkDqULQCEnqtm68Cc0FfKCV+aygL1qRRMHwfttGd82y5GqqnaEejg9Ummb/e7tGIaHsSlQJ9Met3Wwo9CzXCMZUa+SOq2orh0b2dv0Gj0xi4vzjBKdllxE6aXRYgXfq2h7Nxnx3MrdgnyUTn5FEJicPbXU4knlZEXE2+qSSmMeCaXw4KzSF/e5nDilQYgTYxRqE06Qzu1t0xqZQsIHnAdkFjmEdLpFwePjthqUUl2mRr7jGCNqZgmH1u\";\r\n\r\n    /**\r\n     * {@link #vuforia} is the variable we will use to store our instance of the Vuforia\r\n     * localization engine.\r\n     */\r\n    private VuforiaLocalizer vuforia;\r\n\r\n    /**\r\n     * {@link #tfod} is the variable we will use to store our instance of the TensorFlow Object\r\n     * Detection engine.\r\n     */\r\n    private TFObjectDetector tfod;\r\n\r\n    @Override\r\n    public void runOpMode() {\r\n        // The TFObjectDetector uses the camera frames from the VuforiaLocalizer, so we create that\r\n        // first.\r\n        initVuforia();\r\n        initTfod();\r\n\r\n        /**\r\n         * Activate TensorFlow Object Detection before we wait for the start command.\r\n         * Do it here so that the Camera Stream window will have the TensorFlow annotations visible.\r\n         **/\r\n        if (tfod != null) {\r\n            tfod.activate();\r\n\r\n            // The TensorFlow software will scale the input images from the camera to a lower resolution.\r\n            // This can result in lower detection accuracy at longer distances (> 55cm or 22\").\r\n            // If your target is at distance greater than 50 cm (20\") you can adjust the magnification value\r\n            // to artificially zoom in to the center of image.  For best results, the \"aspectRatio\" argument\r\n            // should be set to the value of the images used to create the TensorFlow Object Detection model\r\n            // (typically 16/9).\r\n            tfod.setZoom(2.5, 16.0/9.0);\r\n        }\r\n\r\n        /** Wait for the game to begin */\r\n        telemetry.addData(\">\", \"Press Play to start op mode\");\r\n        telemetry.update();\r\n        waitForStart();\r\n\r\n        if (opModeIsActive()) {\r\n            while (opModeIsActive()) {\r\n                if (tfod != null) {\r\n                    // getUpdatedRecognitions() will return null if no new information is available since\r\n                    // the last time that call was made.\r\n                    List<Recognition> updatedRecognitions = tfod.getUpdatedRecognitions();\r\n                    if (updatedRecognitions != null) {\r\n                      telemetry.addData(\"# Object Detected\", updatedRecognitions.size());\r\n                      // step through the list of recognitions and display boundary info.\r\n                      int i = 0;\r\n                      for (Recognition recognition : updatedRecognitions) {\r\n                        telemetry.addData(String.format(\"label (%d)\", i), recognition.getLabel());\r\n                        telemetry.addData(String.format(\"  left,top (%d)\", i), \"%.03f , %.03f\",\r\n                                recognition.getLeft(), recognition.getTop());\r\n                        telemetry.addData(String.format(\"  right,bottom (%d)\", i), \"%.03f , %.03f\",\r\n                                recognition.getRight(), recognition.getBottom());\r\n                        i++;\r\n                      }\r\n                      telemetry.update();\r\n                    }\r\n                }\r\n            }\r\n        }\r\n    }\r\n\r\n    /**\r\n     * Initialize the Vuforia localization engine.\r\n     */\r\n    private void initVuforia() {\r\n        /*\r\n         * Configure Vuforia by creating a Parameter object, and passing it to the Vuforia engine.\r\n         */\r\n        VuforiaLocalizer.Parameters parameters = new VuforiaLocalizer.Parameters();\r\n\r\n        parameters.vuforiaLicenseKey = VUFORIA_KEY;\r\n        parameters.cameraName = hardwareMap.get(WebcamName.class, \"Webcam 1\");\r\n\r\n        //  Instantiate the Vuforia engine\r\n        vuforia = ClassFactory.getInstance().createVuforia(parameters);\r\n\r\n        // Loading trackables is not necessary for the TensorFlow Object Detection engine.\r\n    }\r\n\r\n    /**\r\n     * Initialize the TensorFlow Object Detection engine.\r\n     */\r\n    private void initTfod() {\r\n        int tfodMonitorViewId = hardwareMap.appContext.getResources().getIdentifier(\r\n            \"tfodMonitorViewId\", \"id\", hardwareMap.appContext.getPackageName());\r\n        TFObjectDetector.Parameters tfodParameters = new TFObjectDetector.Parameters(tfodMonitorViewId);\r\n       tfodParameters.minResultConfidence = 0.8f;\r\n       tfodParameters.isModelTensorFlow2 = true;\r\n       tfodParameters.inputSize = 320;\r\n       tfod = ClassFactory.getInstance().createTFObjectDetector(tfodParameters, vuforia);\r\n       tfod.loadModelFromAsset(TFOD_MODEL_ASSET, LABELS);\r\n    }\r\n}\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/FtcRobotController/src/main/java/org/firstinspires/ftc/robotcontroller/external/samples/ConceptTensorFlowObjectDetectionWebcam.java b/FtcRobotController/src/main/java/org/firstinspires/ftc/robotcontroller/external/samples/ConceptTensorFlowObjectDetectionWebcam.java
--- a/FtcRobotController/src/main/java/org/firstinspires/ftc/robotcontroller/external/samples/ConceptTensorFlowObjectDetectionWebcam.java	(revision e88ab475f7abf8372d216d278ad86bc2d6146a80)
+++ b/FtcRobotController/src/main/java/org/firstinspires/ftc/robotcontroller/external/samples/ConceptTensorFlowObjectDetectionWebcam.java	(date 1635108278067)
@@ -29,6 +29,7 @@
 
 package org.firstinspires.ftc.robotcontroller.external.samples;
 
+import com.acmerobotics.dashboard.FtcDashboard;
 import com.qualcomm.robotcore.eventloop.opmode.Disabled;
 import com.qualcomm.robotcore.eventloop.opmode.LinearOpMode;
 import com.qualcomm.robotcore.eventloop.opmode.TeleOp;
@@ -50,7 +51,7 @@
  * is explained below.
  */
 @TeleOp(name = "Concept: TensorFlow Object Detection Webcam", group = "Concept")
-//@Disabled
+@Disabled
 public class ConceptTensorFlowObjectDetectionWebcam extends LinearOpMode {
   /* Note: This sample uses the all-objects Tensor Flow model (FreightFrenzy_BCDM.tflite), which contains
    * the following 4 detectable objects
@@ -105,6 +106,13 @@
         initVuforia();
         initTfod();
 
+//        VuforiaLocalizer.Parameters vuforiaParams = new VuforiaLocalizer.Parameters(org.firstinspires.ftc.teamcode.R.id.cameraMonitorViewId);
+//        vuforiaParams.vuforiaLicenseKey = VUFORIA_KEY;
+//        vuforiaParams.cameraDirection = VuforiaLocalizer.CameraDirection.BACK;
+//        VuforiaLocalizer vuforia = ClassFactory.getInstance().createVuforia(vuforiaParams);
+
+        FtcDashboard.getInstance().startCameraStream(vuforia, 24);
+
         /**
          * Activate TensorFlow Object Detection before we wait for the start command.
          * Do it here so that the Camera Stream window will have the TensorFlow annotations visible.
